{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRACTURE DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROJECT: https://github.com/iamndlovu/trauma_series_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:12.690390Z",
     "iopub.status.busy": "2022-05-18T14:31:12.690088Z",
     "iopub.status.idle": "2022-05-18T14:31:18.120585Z",
     "shell.execute_reply": "2022-05-18T14:31:18.119787Z",
     "shell.execute_reply.started": "2022-05-18T14:31:12.690361Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set all the Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:18.123615Z",
     "iopub.status.busy": "2022-05-18T14:31:18.123136Z",
     "iopub.status.idle": "2022-05-18T14:31:18.130565Z",
     "shell.execute_reply": "2022-05-18T14:31:18.129613Z",
     "shell.execute_reply.started": "2022-05-18T14:31:18.123575Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS=3\n",
    "EPOCHS=60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data into tensorflow dataset object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use image_dataset_from_directory api to load all images in tensorflow dataset: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:18.132002Z",
     "iopub.status.busy": "2022-05-18T14:31:18.131761Z",
     "iopub.status.idle": "2022-05-18T14:31:23.975725Z",
     "shell.execute_reply": "2022-05-18T14:31:23.974870Z",
     "shell.execute_reply.started": "2022-05-18T14:31:18.131967Z"
    }
   },
   "outputs": [],
   "source": [
    "from struct import unpack\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "marker_mapping = {\n",
    "    0xffd8: \"Start of Image\",\n",
    "    0xffe0: \"Application Default Header\",\n",
    "    0xffdb: \"Quantization Table\",\n",
    "    0xffc0: \"Start of Frame\",\n",
    "    0xffc4: \"Define Huffman Table\",\n",
    "    0xffda: \"Start of Scan\",\n",
    "    0xffd9: \"End of Image\"\n",
    "}\n",
    "\n",
    "\n",
    "class JPEG:\n",
    "    def __init__(self, image_file):\n",
    "        with open(image_file, 'rb') as f:\n",
    "            self.img_data = f.read()\n",
    "    \n",
    "    def decode(self):\n",
    "        data = self.img_data\n",
    "        while(True):\n",
    "            marker, = unpack(\">H\", data[0:2])\n",
    "            # print(marker_mapping.get(marker))\n",
    "            if marker == 0xffd8:\n",
    "                data = data[2:]\n",
    "            elif marker == 0xffd9:\n",
    "                return\n",
    "            elif marker == 0xffda:\n",
    "                data = data[-2:]\n",
    "            else:\n",
    "                lenchunk, = unpack(\">H\", data[2:4])\n",
    "                data = data[2+lenchunk:]            Epoch 1/60\n",
    "119/119 [==============================] - 29s 245ms/step - loss: 0.8141 - accuracy: 0.6478 - val_loss: 0.7950 - val_accuracy: 0.6339\n",
    "Epoch 2/60\n",
    "119/119 [==============================] - 29s 244ms/step - loss: 0.7659 - accuracy: 0.6788 - val_loss: 0.8341 - val_accuracy: 0.6205\n",
    "Epoch 3/60\n",
    "119/119 [==============================] - 29s 247ms/step - loss: 0.7442 - accuracy: 0.6822 - val_loss: 0.6618 - val_accuracy: 0.7210\n",
    "Epoch 4/60\n",
    "119/119 [==============================] - 29s 240ms/step - loss: 0.7260 - accuracy: 0.6972 - val_loss: 0.8722 - val_accuracy: 0.6652\n",
    "Epoch 5/60\n",
    "119/119 [==============================] - 29s 247ms/step - loss: 0.7136 - accuracy: 0.7009 - val_loss: 0.6899 - val_accuracy: 0.7589\n",
    "Epoch 6/60\n",
    "119/119 [==============================] - 29s 245ms/step - loss: 0.6730 - accuracy: 0.7214 - val_loss: 0.7622 - val_accuracy: 0.7188\n",
    "Epoch 7/60\n",
    "119/119 [==============================] - 29s 243ms/step - loss: 0.6755 - accuracy: 0.7198 - val_loss: 0.6175 - val_accuracy: 0.7679\n",
    "Epoch 8/60\n",
    "119/119 [==============================] - 29s 245ms/step - loss: 0.6473 - accuracy: 0.7256 - val_loss: 0.6501 - val_accuracy: 0.7612\n",
    "Epoch 9/60\n",
    "119/119 [==============================] - 29s 246ms/step - loss: 0.6208 - accuracy: 0.7518 - val_loss: 0.5585 - val_accuracy: 0.7701\n",
    "Epoch 10/60\n",
    "119/119 [==============================] - 29s 244ms/step - loss: 0.6156 - accuracy: 0.7377 - val_loss: 0.7614 - val_accuracy: 0.6942\n",
    "Epoch 11/60\n",
    "119/119 [==============================] - 29s 247ms/step - loss: 0.5975 - accuracy: 0.7537 - val_loss: 0.6583 - val_accuracy: 0.7634\n",
    "Epoch 12/60\n",
    "119/119 [==============================] - 29s 241ms/step - loss: 0.5855 - accuracy: 0.7566 - val_loss: 0.5837 - val_accuracy: 0.7500\n",
    "Epoch 13/60\n",
    "119/119 [==============================] - 30s 249ms/step - loss: 0.5618 - accuracy: 0.7658 - val_loss: 0.6750 - val_accuracy: 0.7478\n",
    "Epoch 14/60\n",
    "119/119 [==============================] - 29s 246ms/step - loss: 0.5751 - accuracy: 0.7644 - val_loss: 0.5311 - val_accuracy: 0.7679\n",
    "Epoch 15/60\n",
    "119/119 [==============================] - 29s 245ms/step - loss: 0.5580 - accuracy: 0.7755 - val_loss: 0.5165 - val_accuracy: 0.7835\n",
    "Epoch 16/60\n",
    "119/119 [==============================] - 30s 255ms/step - loss: 0.5255 - accuracy: 0.7847 - val_loss: 0.5180 - val_accuracy: 0.7991\n",
    "Epoch 17/60\n",
    "119/119 [==============================] - 29s 245ms/step - loss: 0.5387 - accuracy: 0.7776 - val_loss: 0.5256 - val_accuracy: 0.7879\n",
    "Epoch 18/60\n",
    "119/119 [==============================] - 29s 244ms/step - loss: 0.5039 - accuracy: 0.7946 - val_loss: 0.7164 - val_accuracy: 0.7567\n",
    "Epoch 19/60\n",
    "119/119 [==============================] - 29s 246ms/step - loss: 0.4988 - accuracy: 0.7965 - val_loss: 0.8025 - val_accuracy: 0.7388\n",
    "Epoch 20/60\n",
    "119/119 [==============================] - 30s 249ms/step - loss: 0.4942 - accuracy: 0.7999 - val_loss: 0.5322 - val_accuracy: 0.8147\n",
    "Epoch 21/60\n",
    "119/119 [==============================] - 30s 254ms/step - loss: 0.4888 - accuracy: 0.8030 - val_loss: 0.4536 - val_accuracy: 0.8080\n",
    "Epoch 22/60\n",
    "119/119 [==============================] - 29s 247ms/step - loss: 0.4916 - accuracy: 0.7962 - val_loss: 0.4809 - val_accuracy: 0.8192\n",
    "Epoch 23/60\n",
    "119/119 [==============================] - 30s 249ms/step - loss: 0.4603 - accuracy: 0.8099 - val_loss: 0.5151 - val_accuracy: 0.7946\n",
    "Epoch 24/60\n",
    "119/119 [==============================] - 29s 246ms/step - loss: 0.4567 - accuracy: 0.8099 - val_loss: 0.6040 - val_accuracy: 0.7879\n",
    "Epoch 25/60\n",
    "119/119 [==============================] - 29s 245ms/step - loss: 0.4694 - accuracy: 0.8028 - val_loss: 0.5190 - val_accuracy: 0.8192\n",
    "Epoch 26/60\n",
    "119/119 [==============================] - 29s 248ms/step - loss: 0.4609 - accuracy: 0.8093 - val_loss: 0.5211 - val_accuracy: 0.7768\n",
    "Epoch 27/60\n",
    "119/119 [==============================] - 29s 245ms/step - loss: 0.4706 - accuracy: 0.8067 - val_loss: 0.6202 - val_accuracy: 0.7634\n",
    "Epoch 28/60\n",
    "119/119 [==============================] - 29s 245ms/step - loss: 0.4328 - accuracy: 0.8283 - val_loss: 0.7511 - val_accuracy: 0.7768\n",
    "Epoch 29/60\n",
    "119/119 [==============================] - 29s 243ms/step - loss: 0.4292 - accuracy: 0.8314 - val_loss: 0.6164 - val_accuracy: 0.7790\n",
    "Epoch 30/60\n",
    "119/119 [==============================] - 29s 243ms/step - loss: 0.4316 - accuracy: 0.8306 - val_loss: 0.5768 - val_accuracy: 0.7879\n",
    "Epoch 31/60\n",
    "119/119 [==============================] - 29s 244ms/step - loss: 0.4215 - accuracy: 0.8248 - val_loss: 0.6536 - val_accuracy: 0.7522\n",
    "Epoch 32/60\n",
    "119/119 [==============================] - 29s 243ms/step - loss: 0.4383 - accuracy: 0.8204 - val_loss: 0.5611 - val_accuracy: 0.7991\n",
    "Epoch 33/60\n",
    "119/119 [==============================] - 30s 248ms/step - loss: 0.4186 - accuracy: 0.8311 - val_loss: 0.4469 - val_accuracy: 0.8281\n",
    "Epoch 34/60\n",
    "119/119 [==============================] - 29s 241ms/step - loss: 0.4224 - accuracy: 0.8311 - val_loss: 0.6275 - val_accuracy: 0.7812\n",
    "Epoch 35/60\n",
    "119/119 [==============================] - 29s 244ms/step - loss: 0.4087 - accuracy: 0.8306 - val_loss: 0.5234 - val_accuracy: 0.7835\n",
    "Epoch 36/60\n",
    "119/119 [==============================] - 29s 246ms/step - loss: 0.4058 - accuracy: 0.8327 - val_loss: 0.5370 - val_accuracy: 0.7634\n",
    "Epoch 37/60\n",
    "119/119 [==============================] - 29s 243ms/step - loss: 0.4024 - accuracy: 0.8380 - val_loss: 0.3788 - val_accuracy: 0.8549\n",
    "Epoch 38/60\n",
    "119/119 [==============================] - 29s 245ms/step - loss: 0.4145 - accuracy: 0.8343 - val_loss: 0.3504 - val_accuracy: 0.8661\n",
    "Epoch 39/60\n",
    "119/119 [==============================] - 30s 249ms/step - loss: 0.3968 - accuracy: 0.8438 - val_loss: 0.4662 - val_accuracy: 0.8438\n",
    "Epoch 40/60\n",
    "119/119 [==============================] - 29s 246ms/step - loss: 0.3896 - accuracy: 0.8419 - val_loss: 0.4496 - val_accuracy: 0.8147\n",
    "Epoch 41/60\n",
    "119/119 [==============================] - 30s 248ms/step - loss: 0.3695 - accuracy: 0.8495 - val_loss: 0.3662 - val_accuracy: 0.8661\n",
    "Epoch 42/60\n",
    "119/119 [==============================] - 29s 245ms/step - loss: 0.3680 - accuracy: 0.8529 - val_loss: 0.6423 - val_accuracy: 0.7902\n",
    "Epoch 43/60\n",
    "119/119 [==============================] - 29s 244ms/step - loss: 0.3614 - accuracy: 0.8571 - val_loss: 0.5981 - val_accuracy: 0.7991\n",
    "Epoch 44/60\n",
    "119/119 [==============================] - 29s 245ms/step - loss: 0.3774 - accuracy: 0.8558 - val_loss: 0.5863 - val_accuracy: 0.7746\n",
    "Epoch 45/60\n",
    "119/119 [==============================] - 29s 245ms/step - loss: 0.3863 - accuracy: 0.8480 - val_loss: 0.7150 - val_accuracy: 0.7612\n",
    "Epoch 46/60\n",
    "119/119 [==============================] - 29s 243ms/step - loss: 0.3712 - accuracy: 0.8524 - val_loss: 0.5173 - val_accuracy: 0.8125\n",
    "Epoch 47/60\n",
    "119/119 [==============================] - 29s 241ms/step - loss: 0.3663 - accuracy: 0.8569 - val_loss: 0.6095 - val_accuracy: 0.7946\n",
    "Epoch 48/60\n",
    "            if len(data)==0:\n",
    "                raise TypeError(\"issue reading jpeg file\")        \n",
    "\n",
    "\n",
    "bads = []\n",
    "# img_dir = '../input/chestpelviscspinescans'\n",
    "\n",
    "# !cp -r ../input/chestpelviscspinescans ./\n",
    "\n",
    "img_dir = './imgs'\n",
    "\n",
    "for dirName, subdirList, fileList in os.walk(img_dir):\n",
    "#     imagesList = fileList\n",
    "#     print(subdirList)\n",
    "    for subdir in subdirList:\n",
    "        subdirPath = os.path.join(dirName,subdir)\n",
    "#         print(subdirPath)\n",
    "        imagesList = os.listdir(subdirPath)\n",
    "#         print(imgList)\n",
    "        for img in tqdm(imagesList):\n",
    "#             print(img)\n",
    "            imagePath = os.path.join(subdirPath,img)\n",
    "#             print(imagePath)\n",
    "            image = JPEG(imagePath) \n",
    "            try:\n",
    "                image.decode()   \n",
    "            except:\n",
    "                bads.append(imagePath)\n",
    "\n",
    "\n",
    "for imagePath in bads:\n",
    "#     print(imagePaths)\n",
    "    os.remove(imagePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:23.978261Z",
     "iopub.status.busy": "2022-05-18T14:31:23.977837Z",
     "iopub.status.idle": "2022-05-18T14:31:27.059220Z",
     "shell.execute_reply": "2022-05-18T14:31:27.057705Z",
     "shell.execute_reply.started": "2022-05-18T14:31:23.978222Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"./imgs\",\n",
    "    seed=123,\n",
    "    shuffle=True,\n",
    "    image_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Watch video below on tensorflow input pipeline first if you don't know about tensorflow datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:27.061008Z",
     "iopub.status.busy": "2022-05-18T14:31:27.060744Z",
     "iopub.status.idle": "2022-05-18T14:31:27.068632Z",
     "shell.execute_reply": "2022-05-18T14:31:27.067948Z",
     "shell.execute_reply.started": "2022-05-18T14:31:27.060974Z"
    }
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/VFEOskzhhbc\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:27.070493Z",
     "iopub.status.busy": "2022-05-18T14:31:27.070036Z",
     "iopub.status.idle": "2022-05-18T14:31:27.082264Z",
     "shell.execute_reply": "2022-05-18T14:31:27.081120Z",
     "shell.execute_reply.started": "2022-05-18T14:31:27.070457Z"
    }
   },
   "outputs": [],
   "source": [
    "class_names = dataset.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:27.083744Z",
     "iopub.status.busy": "2022-05-18T14:31:27.083241Z",
     "iopub.status.idle": "2022-05-18T14:31:31.955767Z",
     "shell.execute_reply": "2022-05-18T14:31:31.955045Z",
     "shell.execute_reply.started": "2022-05-18T14:31:27.083710Z"
    }
   },
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in dataset.take(1):\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some of the images from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:31.957362Z",
     "iopub.status.busy": "2022-05-18T14:31:31.957109Z",
     "iopub.status.idle": "2022-05-18T14:31:37.242276Z",
     "shell.execute_reply": "2022-05-18T14:31:37.240861Z",
     "shell.execute_reply.started": "2022-05-18T14:31:31.957327Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for image_batch, labels_batch in dataset.take(1):\n",
    "    for i in range(12):\n",
    "        ax = plt.subplot(3, 4, i + 1)\n",
    "        plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels_batch[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Split Dataset\n",
    "\n",
    "Dataset should be bifurcated into 3 subsets, namely:\n",
    "1. Training: Dataset to be used while training\n",
    "2. Validation: Dataset to be tested against while training\n",
    "3. Test: Dataset to be tested against after we trained a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:37.243492Z",
     "iopub.status.busy": "2022-05-18T14:31:37.243263Z",
     "iopub.status.idle": "2022-05-18T14:31:37.253617Z",
     "shell.execute_reply": "2022-05-18T14:31:37.252638Z",
     "shell.execute_reply.started": "2022-05-18T14:31:37.243465Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    \n",
    "    ds_size = len(ds)\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:37.257588Z",
     "iopub.status.busy": "2022-05-18T14:31:37.257342Z",
     "iopub.status.idle": "2022-05-18T14:31:37.269691Z",
     "shell.execute_reply": "2022-05-18T14:31:37.268858Z",
     "shell.execute_reply.started": "2022-05-18T14:31:37.257554Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:37.271731Z",
     "iopub.status.busy": "2022-05-18T14:31:37.271259Z",
     "iopub.status.idle": "2022-05-18T14:31:37.278066Z",
     "shell.execute_reply": "2022-05-18T14:31:37.277347Z",
     "shell.execute_reply.started": "2022-05-18T14:31:37.271691Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:37.280289Z",
     "iopub.status.busy": "2022-05-18T14:31:37.279641Z",
     "iopub.status.idle": "2022-05-18T14:31:37.286677Z",
     "shell.execute_reply": "2022-05-18T14:31:37.285866Z",
     "shell.execute_reply.started": "2022-05-18T14:31:37.280249Z"
    }
   },
   "outputs": [],
   "source": [
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:37.288768Z",
     "iopub.status.busy": "2022-05-18T14:31:37.288273Z",
     "iopub.status.idle": "2022-05-18T14:31:37.295790Z",
     "shell.execute_reply": "2022-05-18T14:31:37.294922Z",
     "shell.execute_reply.started": "2022-05-18T14:31:37.288731Z"
    }
   },
   "outputs": [],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache, Shuffle, and Prefetch the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:37.297853Z",
     "iopub.status.busy": "2022-05-18T14:31:37.297325Z",
     "iopub.status.idle": "2022-05-18T14:31:37.308906Z",
     "shell.execute_reply": "2022-05-18T14:31:37.308110Z",
     "shell.execute_reply.started": "2022-05-18T14:31:37.297814Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Layer for Resizing and Normalization\n",
    "Before we feed our images to network, we should be resizing it to the desired size. \n",
    "Moreover, to improve model performance, we should normalize the image pixel value (keeping them in range 0 and 1 by dividing by 256).\n",
    "This should happen while training as well as inference. Hence we can add that as a layer in our Sequential Model.\n",
    "\n",
    "You might be thinking why do we need to resize (256,256) image to again (256,256). You are right we don't need to but this will be useful when we are done with the training and start using the model for predictions. At that time somone can supply an image that is not (256,256) and this layer will resize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:37.310872Z",
     "iopub.status.busy": "2022-05-18T14:31:37.310507Z",
     "iopub.status.idle": "2022-05-18T14:31:37.346677Z",
     "shell.execute_reply": "2022-05-18T14:31:37.345775Z",
     "shell.execute_reply.started": "2022-05-18T14:31:37.310836Z"
    }
   },
   "outputs": [],
   "source": [
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "Data Augmentation is needed when we have less data, this boosts the accuracy of our model by augmenting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:37.351293Z",
     "iopub.status.busy": "2022-05-18T14:31:37.351019Z",
     "iopub.status.idle": "2022-05-18T14:31:37.373062Z",
     "shell.execute_reply": "2022-05-18T14:31:37.372362Z",
     "shell.execute_reply.started": "2022-05-18T14:31:37.351256Z"
    }
   },
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Data Augmentation to Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:37.374357Z",
     "iopub.status.busy": "2022-05-18T14:31:37.374116Z",
     "iopub.status.idle": "2022-05-18T14:31:37.688814Z",
     "shell.execute_reply": "2022-05-18T14:31:37.688051Z",
     "shell.execute_reply.started": "2022-05-18T14:31:37.374325Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(\n",
    "    lambda x, y: (data_augmentation(x, training=True), y)\n",
    ").prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Watch below video if you are not familiar with data augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:37.690699Z",
     "iopub.status.busy": "2022-05-18T14:31:37.690219Z",
     "iopub.status.idle": "2022-05-18T14:31:37.697557Z",
     "shell.execute_reply": "2022-05-18T14:31:37.696821Z",
     "shell.execute_reply.started": "2022-05-18T14:31:37.690661Z"
    }
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/mTVf7BN7S8w\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "We use a CNN coupled with a Softmax activation in the output layer. We also add the initial layers for resizing, normalization and Data Augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are going to use convolutional neural network (CNN) here. CNN is popular for image classification tasks. Watch below video to understand fundamentals of CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:37.700959Z",
     "iopub.status.busy": "2022-05-18T14:31:37.699965Z",
     "iopub.status.idle": "2022-05-18T14:31:37.708389Z",
     "shell.execute_reply": "2022-05-18T14:31:37.707666Z",
     "shell.execute_reply.started": "2022-05-18T14:31:37.700340Z"
    }
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/zfiSAzpy9NM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:37.710488Z",
     "iopub.status.busy": "2022-05-18T14:31:37.709691Z",
     "iopub.status.idle": "2022-05-18T14:31:37.859242Z",
     "shell.execute_reply": "2022-05-18T14:31:37.858522Z",
     "shell.execute_reply.started": "2022-05-18T14:31:37.710291Z"
    }
   },
   "outputs": [],
   "source": [
    "input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "n_classes = len(class_names)\n",
    "\n",
    "model = models.Sequential([\n",
    "    resize_and_rescale,\n",
    "    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.build(input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:37.861038Z",
     "iopub.status.busy": "2022-05-18T14:31:37.860341Z",
     "iopub.status.idle": "2022-05-18T14:31:37.881572Z",
     "shell.execute_reply": "2022-05-18T14:31:37.880937Z",
     "shell.execute_reply.started": "2022-05-18T14:31:37.861000Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model\n",
    "We use `adam` Optimizer, `SparseCategoricalCrossentropy` for losses, `accuracy` as a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:31:37.883196Z",
     "iopub.status.busy": "2022-05-18T14:31:37.882571Z",
     "iopub.status.idle": "2022-05-18T14:31:37.905796Z",
     "shell.execute_reply": "2022-05-18T14:31:37.905067Z",
     "shell.execute_reply.started": "2022-05-18T14:31:37.883160Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T14:40:10.012346Z",
     "iopub.status.busy": "2022-05-18T14:40:10.011786Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=val_ds,\n",
    "    verbose=1,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-18T14:39:48.243958Z",
     "iopub.status.idle": "2022-05-18T14:39:48.244601Z",
     "shell.execute_reply": "2022-05-18T14:39:48.244361Z",
     "shell.execute_reply.started": "2022-05-18T14:39:48.244332Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Accuracy and Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-18T14:39:48.245831Z",
     "iopub.status.idle": "2022-05-18T14:39:48.246428Z",
     "shell.execute_reply": "2022-05-18T14:39:48.246211Z",
     "shell.execute_reply.started": "2022-05-18T14:39:48.246183Z"
    }
   },
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-18T14:39:48.247528Z",
     "iopub.status.idle": "2022-05-18T14:39:48.248140Z",
     "shell.execute_reply": "2022-05-18T14:39:48.247912Z",
     "shell.execute_reply.started": "2022-05-18T14:39:48.247871Z"
    }
   },
   "outputs": [],
   "source": [
    "type(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-18T14:39:48.249248Z",
     "iopub.status.idle": "2022-05-18T14:39:48.249851Z",
     "shell.execute_reply": "2022-05-18T14:39:48.249623Z",
     "shell.execute_reply.started": "2022-05-18T14:39:48.249578Z"
    }
   },
   "outputs": [],
   "source": [
    "history.history['loss'][:5] # show loss for first 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-18T14:39:48.250966Z",
     "iopub.status.idle": "2022-05-18T14:39:48.251550Z",
     "shell.execute_reply": "2022-05-18T14:39:48.251335Z",
     "shell.execute_reply.started": "2022-05-18T14:39:48.251308Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-18T14:39:48.252665Z",
     "iopub.status.idle": "2022-05-18T14:39:48.253252Z",
     "shell.execute_reply": "2022-05-18T14:39:48.253031Z",
     "shell.execute_reply.started": "2022-05-18T14:39:48.253005Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(EPOCHS), acc, label='Training Accuracy')\n",
    "plt.plot(range(EPOCHS), val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(EPOCHS), loss, label='Training Loss')\n",
    "plt.plot(range(EPOCHS), val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run prediction on a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-18T14:39:48.254352Z",
     "iopub.status.idle": "2022-05-18T14:39:48.254933Z",
     "shell.execute_reply": "2022-05-18T14:39:48.254700Z",
     "shell.execute_reply.started": "2022-05-18T14:39:48.254673Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for images_batch, labels_batch in test_ds.take(1):\n",
    "    \n",
    "    first_image = images_batch[0].numpy().astype('uint8')\n",
    "    first_label = labels_batch[0].numpy()\n",
    "    \n",
    "    print(\"first image to predict\")\n",
    "    plt.imshow(first_image)\n",
    "    print(\"actual label:\",class_names[first_label])\n",
    "    \n",
    "    batch_prediction = model.predict(images_batch)\n",
    "    print(\"predicted label:\",class_names[np.argmax(batch_prediction[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-18T14:39:48.256046Z",
     "iopub.status.idle": "2022-05-18T14:39:48.256618Z",
     "shell.execute_reply": "2022-05-18T14:39:48.256404Z",
     "shell.execute_reply.started": "2022-05-18T14:39:48.256379Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, img):\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
    "    return predicted_class, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now run inference on few sample images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-18T14:39:48.257726Z",
     "iopub.status.idle": "2022-05-18T14:39:48.258308Z",
     "shell.execute_reply": "2022-05-18T14:39:48.258094Z",
     "shell.execute_reply.started": "2022-05-18T14:39:48.258061Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "for images, labels in test_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        \n",
    "        predicted_class, confidence = predict(model, images[i].numpy())\n",
    "        actual_class = class_names[labels[i]] \n",
    "        \n",
    "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
    "        \n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model\n",
    "We append the model to the list of models as a new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-18T14:39:48.259376Z",
     "iopub.status.idle": "2022-05-18T14:39:48.259919Z",
     "shell.execute_reply": "2022-05-18T14:39:48.259703Z",
     "shell.execute_reply.started": "2022-05-18T14:39:48.259679Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"./models/trauma_series_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
